![image](https://github.com/JiabinO/Code/assets/154659312/7e2760c3-8dc1-4ce6-978c-7c02f9de44c0)# 香橙派开发基础环境配置
```
tips: 首先默认你手上的香橙派是已经拼装好的，并且品质没问题。
```
## 香橙派的内核烧录与安装
至少准备以下东西：
| 配件名称                           | 描述                                                                                         | 推荐型号                                                                                   |
| ---------------------------------- | -------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------ |
| 香橙派AI Pro                       | 香橙派AI Pro主板，或包含电源（64W）、散热组件、AI Pro金属外壳等。                             | Orange Pi AI Pro主板：8TOPS算力，16GB LPDDR4X                                              |
| Micro SD卡（TF卡）                 | Micro SD卡（TF卡）用于装载镜像运行开发板。                                                   | 推荐使用SD 3.0接口标准的Micro SD卡，容量推荐不小于64GB。                                   |
| 读卡器                             | 用于插入Micro SD卡烧录镜像                                                                   | 需使用支持Micro SD卡的读卡器。读卡器的接口可以根据PC接口配置选择USB或Type-C接口。          |
| PC（笔记本或台式机）               | 用于安装制卡工具、烧录镜像。                                                                 | 操作系统：Windows 10、Windows 11。                                                         |
| MicroUSB连接线（可选）             | 以串口的方式连接开发板和PC                                                                   | 非SSH登录，部分操作受限                                                                    |                                                                 |

启动前先确保香橙派的主板背面的两个启动方式拨码开关都是**靠右的**，否则内核启动可能由于其配置不同，启动的方式不符合预期。
![image](https://github.com/JiabinO/Code/assets/154659312/22318131-1179-4918-8f6d-4910884b4db6)
### 相关工具的安装
- 香橙派对应的Ubuntu镜像下载地址：http://www.orangepi.cn/html/hardWare/computerAndMicrocontrollers/service-and-support/Orange-Pi-AIpro.html
  
  下载过慢可以通过这个链接下载：
  https://obs-9be7.obs.cn-east-2.myhuaweicloud.com/OrangePi/20240318/opiaipro_ubuntu22.04_desktop_aarch64_20240318.img.xz
  
  （下载下来是这个文件，需要解压缩）
  ![image](https://github.com/JiabinO/Code/assets/154659312/29991190-84a0-4787-9523-91692276bdcf)
- balenaEtcher制卡工具下载地址：https://obs-9be7.obs.cn-east-2.myhuaweicloud.com/OrangePi/balenaEtcher/balenaEtcher-Setup-1.18.4.exe
  
  <img alt="image" src="https://github.com/JiabinO/Code/assets/154659312/91320ee5-b926-4071-bbba-c2663e8bee26">

将解压后的镜像作为烧录的原镜像，烧录地址选择读卡器，烧录过程大概十五分钟左右。
烧录完成后，将TF卡插入到香橙派的读卡器。

- 镜像运行工具Mobatek：https://download.mobatek.net/2222022102210348/MobaXterm_Portable_v22.2.zip
  安装后解压，运行可执行文件：
  ![image](https://github.com/JiabinO/Code/assets/154659312/5e186b20-a1e9-40ca-8365-7c669c3d40e4)

### 启动Ubuntu镜像
1. 打开镜像执行程序后，首先点击"Session"
   ![image](https://github.com/JiabinO/Code/assets/154659312/6583a683-9c6a-4693-bf1a-eb9c3bd2fdca)
2. 选择串口选项：
   ![image](https://github.com/JiabinO/Code/assets/154659312/c54b8b7b-16cf-4ab1-9938-f2e720254a61)
3. 选择串口类型（名称可能由于设备不同而不同），将波特率设置为115200：
  ![image](https://github.com/JiabinO/Code/assets/154659312/366d3094-922a-4dd8-a1d9-55d9b3b9b624)

   如果找不到串口，点击https://obs-9be7.obs.cn-east-2.myhuaweicloud.com/OrangePi/private/CH343SER.EXE 安装USB转TTL串口芯片的驱动。
4. 点击OK后，按下重启按钮，屏幕会输出这些信息：
   ![image](https://github.com/JiabinO/Code/assets/154659312/9a96ec0f-d534-4cf9-9eb8-e143bb3aed0b)
  等待其启动完毕。
5. 输入用户名密码时，用户名为HwHiAiUser，密码默认为Mind@123.
   ![image](https://github.com/JiabinO/Code/assets/154659312/b85a7dea-57a2-4d37-add2-1e8c46091e65)
6. 登录成功会显示如下界面：
   ![image](https://github.com/JiabinO/Code/assets/154659312/ca5e7550-b6cf-42cf-8719-e574cd4ad92d)
7. 连接网络：
   sudo nmcli dev wifi connect wifi_name password wifi_passwd
   ![image](https://github.com/JiabinO/Code/assets/154659312/0ca59c04-4115-4535-87ea-1204435cb7a7)
8. 配置远程SSH访问环境：
   使用vscode:保证PC和香橙派连接到同一个局域网(ip相同)后，在vscode中输入ssh HwHiAiUser@ipaddr，进行链接即可；
   使用MobaXtrem：点击Session，点击SSH，配置如下：
   ![image](https://github.com/JiabinO/Code/assets/154659312/4ee647c0-c336-4ddd-a763-c63c72f3bdd6)
   然后输入密码后，会在左侧出现文件管理栏。
   ![image](https://github.com/JiabinO/Code/assets/154659312/d980b689-8331-4677-bb41-69810161798b)

# 开发案例：进行cifar数据集的分类
## 获取数据集
1. 使用`wget https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz`命令下载数据集的压缩包
2. 解压缩压缩包` tar -xzvf cifar-10-binary.tar.gz `
   ![image](https://github.com/JiabinO/Code/assets/154659312/81714331-3fd1-4e00-b8bb-589769720a40)
3. 将测试集复制一份出来用于验证：
    - 创建一个用于存放测试文件的文件夹： `mkdir cifar-10-verify-bin`
    - 将数据集中测试集复制一份到改文件夹下： `cp cifar-10-batches-bin/test_batch.bin cifar-10-verify-bin/`
   ![image](https://github.com/JiabinO/Code/assets/154659312/020423f4-c9c6-42c4-abb4-ea0bcbe3c54e)

## 数据的处理
通常需要使用某种手段将数据整理成数据集，以某种结构方式进行存储。这里在dataset_create函数中，使用`data = ds.Cifar10Dataset(data_path)`的库函数进行数据集的创建，当然也可以使用自定义的方式进行数据集的创建。除此之外，为了保证模型的泛化，通常需要将数据进行打乱。这里用到了库中的shuffle函数，将数据集中的数据打乱重排。
除此之外，还需要定义训练或者推理模式下的算子列表，如数据的增强（随机裁剪、翻转等）、通道转换、数据标准化等。
附带一些常见算子的作用：

### 数据标准化
如果在训练机器学习模型时没有对输入数据进行标准化或归一化处理，可能会对梯度下降等优化算法产生以下影响：

1. 收敛速度变慢：不同特征的尺度差异大，导致每个特征对模型损失函数的影响程度不均衡。这会使得优化算法在参数空间中搜索的路径不够直接，导致需要更多的迭代次数才能收敛到最优解。

2. 优化过程不稳定：尺度差异大的特征会导致损失函数的曲面非均匀，出现大幅度的波动或振荡，这可能会使优化算法难以稳定地更新模型参数。

3. 梯度计算不准确：某些特征值过大或过小，可能会使得梯度值非常大或非常小，这会影响梯度下降算法的计算精度和稳定性。

4. 陷入局部最优解或鞍点：尺度差异大可能会导致优化算法更容易陷入局部最优解或鞍点，而不是全局最优解。

5. 模型性能下降：最终训练出的模型可能泛化能力较差，因为模型在训练集上表现良好，但在测试集或实际应用中表现不佳。

### 数据增强
数据增强是在训练机器学习模型时一种常用的技术，通过对原始数据进行随机变换或扩充，从而增加数据的多样性和丰富性。数据增强的主要作用包括以下几个方面：
1. 提升模型的泛化能力：

数据增强可以通过引入随机性变换，使模型在训练过程中接触到更多的数据样本，从而帮助模型学习更加鲁棒和泛化能力强的特征。这有助于减少模型对训练数据的过拟合。

2. 增加数据样本的多样性：
数据增强可以生成多样化的数据样本，例如在图像分类中进行随机旋转、裁剪、翻转等操作，或者在文本处理中进行随机遮蔽、替换等操作。这种多样性有助于模型更好地捕捉数据集中的各种变化和不确定性。

2. 提升模型的鲁棒性：

引入数据增强可以使模型更加鲁棒，即使在面对输入数据中存在的小变化或扰动时，模型也能保持稳定的性能。例如，在图像分类任务中，模型在训练时接触到不同角度、不同光照条件下的图像，可以更好地适应实际应用中的各种情况。

3. 有效利用有限数据集：

在数据集有限或者数据获取成本较高的情况下，数据增强可以通过扩充数据样本，从而提升训练效果。通过合理的数据增强技术，可以在一定程度上缓解数据不足带来的问题。

### 数据的分批
执行`data = data.batch(batch_size, drop_remainder=True)`将数据进行分批，便于训练过程中同时处理多个样本。

### 数据的重复
执行`data = data.repeat(repeat_num)`，用于添加数据副本到原来的训练数据集，便于模型多次看到每个样本以便有效地学习，但是注意重复不能过多，否则可能造成过拟合的问题。

除此之外，还有以下及其它的数据集预处理操作：
1. 特征缩放（Feature Scaling）：
对于某些机器学习算法（如支持向量机、K近邻等），特征的尺度差异会影响模型的性能。因此，常用的方法是对特征进行缩放，使其具有相似的尺度。常见的缩放方法包括 Min-Max 缩放和标准化（Z-score 缩放）。

2. 特征选择（Feature Selection）：
在拥有大量特征的数据集中，可能存在一些冗余或不相关的特征，这些特征可能会影响模型的效果并增加计算复杂度。因此，特征选择的目标是筛选出对目标变量预测有用的特征。常用的方法包括基于统计指标（如方差、相关性）的特征选择和基于模型的特征选择（如递归特征消除）。

3. 数据平衡（Data Balancing）：
在处理不平衡数据集（例如正负样本比例差异较大的分类问题）时，可以采取数据平衡的策略，以确保模型在训练过程中能够充分学习到少数类别的特征。常见的方法包括过采样（如 SMOTE）和欠采样（如随机欠采样）。

4. 数据清洗（Data Cleaning）：
数据集可能存在缺失值、异常值或错误值，这些数据会影响模型的训练和预测。因此，在训练模型之前，通常需要对数据进行清洗和处理，以确保数据的质量和一致性。

5. 数据转换（Data Transformation）：
在一些情况下，原始数据可能不符合模型的假设或要求，需要进行数据转换。例如，对数转换、幂转换或非线性转换可以使数据更符合线性模型的假设，或者提升特定模型的性能。

6. 特征工程（Feature Engineering）：
特征工程是指根据领域知识和数据理解创建新的特征或特征组合，以提高模型性能。良好的特征工程可以帮助模型更好地捕捉数据的本质特征，从而提升预测精度和模型解释性。

## 神经网络的构建
在数据集处理完后，需要定义神经网络的各个步骤。在神经网络类（继承了nn.Cell类）中可以在构造函数中定义相关的算子（如不同参数下的卷积操作、池化、激活、全连接等），并且可以定义神经网络每层执行操作的顺序。

## 模型的训练
模型的训练包括神经网络的计算、损失函数计算、优化器对参数进行优化、调参后继续新一轮训练，直至训练轮次到达目标值时停止。中间包括一些训练指标的日志生成，如训练时间、损失函数的值等，需要通过一些接口进行保存。
![image](https://github.com/JiabinO/Code/assets/154659312/dd3dbde8-533b-42fb-9b5c-0a1162bad4dd)

模型训练的整个代码如下：
```py
import mindspore

# mindspore.dataset
import mindspore.dataset as ds # 数据集的载入
import mindspore.dataset.transforms.c_transforms as C # 常用转化算子
import mindspore.dataset.vision.c_transforms as CV # 图像转化算子

# mindspore.common
from mindspore.common import dtype as mstype # 数据形态转换
from mindspore.common.initializer import Normal # 参数初始化

# mindspore.nn
import mindspore.nn as nn # 各类网络层都在nn里面
from mindspore.nn.metrics import Accuracy, Loss # 测试模型用

# mindspore.train.callback
from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor, Callback # 回调函数


from mindspore import Model # 承载网络结构
from mindspore import save_checkpoint, load_checkpoint # 保存与读取最佳参数
from mindspore import context # 设置mindspore运行的环境


import numpy as np # numpy
import matplotlib.pyplot as plt # 可视化用
import copy # 保存网络参数用

# 数据路径处理
import os, stat   
def create_dataset(data_path, batch_size = 32, repeat_num=1, usage = 'train'):
    """ 
    数据处理
    
    Args:
        data_path (str): 数据路径
        batch_size (int): 批量大小
        usage (str): 训练或测试
        
    Returns:
        Dataset对象
    """
    
    # 载入数据集
    data = ds.Cifar10Dataset(data_path)
    
    # 打乱数据集
    data = data.shuffle(buffer_size=10000)
    
    # 定义算子
    if usage=='train':
        trans = [
            CV.Normalize(RGB_mean, RGB_std), # 数据标准化

            # 数据增强
            CV.RandomCrop([32, 32], [4, 4, 4, 4]), # 随机裁剪
            CV.RandomHorizontalFlip(), # 随机翻转

            CV.HWC2CHW() # 通道前移（为配适网络，CHW的格式可最佳发挥昇腾芯片算力）
        ]
    else:
        trans = [
            CV.Normalize(RGB_mean, RGB_std), # 数据标准化
            CV.HWC2CHW() # 通道前移（为配适网络，CHW的格式可最佳发挥昇腾芯片算力）
        ]
    
    typecast_op = C.TypeCast(mstype.int32) # 原始数据的标签是unint，计算损失需要int

    # 算子运算，应用类型转换算子将原来可能不符合处理接口类型的数据变为符合的数据
    data = data.map(input_columns='label', operations=typecast_op)
    data = data.map(input_columns='image', operations=trans)
    
    # 对原始数据进行分批，分批后data维度变高一维，该维表示为批次
    data = data.batch(batch_size, drop_remainder=True)
    
    # 重复
    data = data.repeat(repeat_num)
    
    return data 

class LeNet5(nn.Cell):
    """
    LeNet5网络

    Args:
        num_class (int): 输出分类数
        num_channel (int): 输入通道数
    Returns:
        Tensor, 输出张量

    Examples:
        >>> LeNet5(10, 3)
    """
    
    # 定义算子
    def __init__(self, num_class=10, num_channel=3):
        super(LeNet5, self).__init__()
        # 卷积层
        self.conv1 = nn.Conv2d(num_channel, 6, 5, pad_mode='valid')
        self.conv2 = nn.Conv2d(6, 16, 5, pad_mode='valid')
        
        # 全连接层
        self.fc1 = nn.Dense(16 * 5 * 5, 120, weight_init=Normal(0.02))
        self.fc2 = nn.Dense(120, 84, weight_init=Normal(0.02))
        self.fc3 = nn.Dense(84, num_class, weight_init=Normal(0.02))
        
        # 激活函数
        self.relu = nn.ReLU()
        
        # 最大池化成
        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)
        
        # 网络展开
        self.flatten = nn.Flatten()

    # 建构网络
    def construct(self, x):
        x = self.conv1(x)
        x = self.relu(x)
        x = self.max_pool2d(x)
        x = self.conv2(x)
        x = self.relu(x)
        x = self.max_pool2d(x)
        x = self.flatten(x)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.relu(x)
        x = self.fc3(x)
        return x 

# 记录模型每个epoch的loss
class TrainHistroy(Callback):
    """
    记录模型训练时每个epoch的loss的回调函数

    Args:
        history (list): 传入list以保存模型每个epoch的loss
    """
    
    def __init__(self, history):
        super(TrainHistroy, self).__init__()
        self.history = history
        
    # 每个epoch结束时执行
    def epoch_end(self, run_context):
        cb_params = run_context.original_args()
        loss = cb_params.net_outputs.asnumpy()
        self.history.append(loss)
        

# 测试并记录模型在测试集的loss和accuracy，每个epoch结束时进行模型测试并记录结果，跟踪并保存准确率最高的模型网络参数
class EvalHistory(Callback):
    """
    记录模型训练时每个epoch在测试集的loss和accuracy的回调函数，并保存准确率最高的模型网络参数

    Args:
        model (Cell): 模型，评估loss和accuracy用
        loss_history (list): 传入list以保存模型每个epoch在测试集的loss
        acc_history (list): 传入list以保存模型每个epoch在测试集的accuracy
        eval_data (Dataset): 测试集，评估模型loss和accuracy用
    """
    
    #保存accuracy最高的网络参数
    best_param = None
    
    def __init__(self, model, loss_history, acc_history, eval_data):
        super(EvalHistory, self).__init__()
        self.loss_history = loss_history
        self.acc_history = acc_history
        self.eval_data = eval_data
        self.model = model
    
    # 每个epoch结束时执行
    def epoch_end(self, run_context):
        cb_params = run_context.original_args()
        res = self.model.eval(self.eval_data, dataset_sink_mode=False)
        
        if len(self.acc_history)==0 or res['accuracy']>=max(self.acc_history):
            self.best_param = copy.deepcopy(cb_params.network)
            
        self.loss_history.append(res['loss'])
        self.acc_history.append(res['accuracy'])
        
        print('acc_eval: ',res['accuracy'])
    
    # 训练结束后执行
    def end(self, run_context):
        # 保存最优网络参数
        best_param_path = os.path.join(ckpt_path, 'best_param.ckpt')
        
        if os.path.exists(best_param_path):
            # best_param.ckpt已存在时MindSpore会覆盖旧的文件，这里修改文件读写权限防止报错
            os.chmod(best_param_path, stat.S_IWRITE)
            
        save_checkpoint(self.best_param, best_param_path) 

# 定义loss记录绘制函数
def plot_loss(hist):
    plt.plot(hist['loss'], marker='.')
    plt.plot(hist['loss_eval'], marker='.')
    plt.title('loss record')
    plt.xlabel('epoch')
    plt.ylabel('loss')
    plt.grid()
    plt.legend(['loss_train', 'loss_eval'], loc='upper right')
    plt.show()
    plt.close()

def plot_accuracy(hist):
    plt.plot(hist['acc_eval'], marker='.')
    plt.title('accuracy history')
    plt.xlabel('epoch')
    plt.ylabel('acc_eval')
    plt.grid()
    plt.show()
    plt.close()

# main
device_target = context.get_context('device_target') 
# 获取运行装置（CPU，GPU，Ascend）
dataset_sink_mode = True if device_target in ['Ascend','GPU'] else False 
# 是否将数据通过pipeline下发到装置上
context.set_context(mode = context.GRAPH_MODE, device_target = device_target) 
# 设置运行环境，静态图context.GRAPH_MODE指向静态图模型，即在运行之前会把全部图建立编译完毕

print(f'device_target: {device_target}')
print(f'dataset_sink_mode: {dataset_sink_mode}') 
# 数据路径
train_path = os.path.join('cifar-10-batches-bin') # 训练集路径
test_path = os.path.join('cifar-10-verify-bin') # 测试集路径
print(f'训练集路径：{train_path}')
print(f'测试集路径：{test_path}') 
category_dict = {0:'airplane',1:'automobile',2:'bird',3:'cat',4:'deer',5:'dog',
                 6:'frog',7:'horse',8:'ship',9:'truck'}

# 载入展示用数据
demo_data = ds.Cifar10Dataset(test_path)

# 设置图像大小
plt.figure(figsize=(6, 6))

# 打印9张子图
i = 1
for dic in demo_data.create_dict_iterator():
    plt.subplot(3,3,i)
    plt.imshow(dic['image'].asnumpy()) # asnumpy：将 MindSpore tensor 转换成 numpy
    plt.axis('off')
    plt.title(category_dict[dic['label'].asnumpy().item()])
    i +=1
    if i > 9 :
        break

plt.show() 

ds_train = ds.Cifar10Dataset(train_path)
#计算数据集平均数和标准差，数据标准化时使用
tmp = np.asarray( [x['image'] for x in ds_train.create_dict_iterator(output_numpy=True)] )
RGB_mean = tuple(np.mean(tmp, axis=(0, 1, 2)))
RGB_std = tuple(np.std(tmp, axis=(0, 1, 2)))

print(RGB_mean)
print(RGB_std) 

train_data = create_dataset(train_path, batch_size = 32, usage = 'train') # 训练数据集
test_data = create_dataset(test_path, batch_size = 50, usage= 'test') # 测试数据集 

# 网络
network1 = LeNet5(10) # 共分成10类

# 损失函数
net_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')

# 优化器
net_opt = nn.Momentum(params=network1.trainable_params(), learning_rate=0.01, momentum=0.9)

# 模型
model = Model(network = network1, loss_fn=net_loss, optimizer=net_opt, metrics={'accuracy': Accuracy(), 'loss':Loss()}) 

ckpt_path = os.path.join('.','results') # 网络参数保存路径
hist = {'loss':[], 'loss_eval':[], 'acc_eval':[]} # 训练过程记录

# 网络参数自动保存，这里设定每2000个step保存一次，最多保存10次
config_ck = CheckpointConfig(save_checkpoint_steps=2000,
                             keep_checkpoint_max=10)

ckpoint_cb = ModelCheckpoint(prefix='checkpoint_lenet', directory=ckpt_path, config=config_ck)

# 监控每次迭代的时间
time_cb = TimeMonitor(data_size=ds_train.get_dataset_size())

# 监控loss值
loss_cb = LossMonitor(per_print_times=500)

# 记录每次迭代的模型损失值
train_hist_cb = TrainHistroy(hist['loss'])

# 测试并记录模型在验证集的loss和accuracy，并保存最优网络参数
eval_hist_cb = EvalHistory(model = model,
                           loss_history = hist['loss_eval'], 
                           acc_history = hist['acc_eval'], 
                           eval_data = test_data) 

epoch = 10 # 迭代次数
# 开始训练
model.train(epoch, train_data, callbacks=[train_hist_cb, eval_hist_cb, time_cb, ckpoint_cb, loss_cb], dataset_sink_mode=dataset_sink_mode) 

plot_loss(hist) 

plot_accuracy(hist) 

# 使用准确率最高的参数组合建立模型，并测试其在验证集上的效果
load_checkpoint(os.path.join(ckpt_path, 'best_param.ckpt'), net=network1)
res = model.eval(test_data, dataset_sink_mode=dataset_sink_mode)
print(res) 
```
## 模型的改进
当然，对于第一个神经网络结构，训练结果表明最终的训练效果不是那么突出，大概在60%左右的正确率，这说明第一个神经网络结构还有提升的空间。
![image](https://github.com/JiabinO/Code/assets/154659312/60acb398-ad31-4c37-9867-1e685fa2c6a4)
本案例给出了这个模型的一个改进版本：
```py
class LeNet5_2(nn.Cell):
    
    # 定义算子
    def __init__(self, num_class=10, num_channel=3):
        super(LeNet5_2, self).__init__()
        self.conv1 = nn.Conv2d(num_channel, 32, 3, pad_mode='valid', weight_init=Normal(0.02))
        self.conv2 = nn.Conv2d(32, 64, 3, pad_mode='valid', weight_init=Normal(0.02))
        self.conv3 = nn.Conv2d(64, 128, 3, pad_mode='valid', weight_init=Normal(0.02))
        self.fc1 = nn.Dense(128 * 2 * 2, 120, weight_init=Normal(0.02))
        self.fc2 = nn.Dense(120, 84, weight_init=Normal(0.02))
        self.fc3 = nn.Dense(84, num_class, weight_init=Normal(0.02))
        self.relu = nn.ReLU()
        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)
        self.flatten = nn.Flatten()
        self.num_class = num_class
    
    # 构建网络
    def construct(self, x):
        x = self.conv1(x)
        x = self.relu(x)
        x = self.max_pool2d(x)
        x = self.conv2(x)
        x = self.relu(x)
        x = self.max_pool2d(x)
        x = self.conv3(x)
        x = self.relu(x)
        x = self.max_pool2d(x)
        x = self.flatten(x)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.relu(x)
        x = self.fc3(x)
        return x
```
相比原来的神经网络，改进后的神经网络做了如下改动：
- 卷积层参数的变化：卷积层的大小减小（由5变为3），这意味着神经网络可以提取更加细致的局部特征；同时通道数（卷积核数）增加到32，这意味着卷积层能够学习到更多的特定特征，提高网络的特征表达能力和整体性能。
- 池化层参数的变化：池化层种类从平均池化变为最大池化，保留了局部区域最显著的特征（但是丢失会某些细节信息以及牺牲图像的平滑性），
